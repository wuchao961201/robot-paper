# PaLM-E: An Embodied Multimodal Language Model

## 基本信息
- **标题**: PaLM-E: An Embodied Multimodal Language Model
- **作者**: Danny Driess, Fei Xia, Mehdi S. M. Sajjadi等
- **发表时间**: 2023
- **会议/期刊**: ICML 2023
- **DOI/链接**: https://proceedings.mlr.press/v202/driess23a.html
- **代码**: 未公开

## 核心贡献
PaLM-E将视觉嵌入融入大型语言模型中，创建了一个能够处理多模态输入并解决具身推理任务的单一模型。这种方法使模型能够将来自不同传感器的信息整合到端到端的基础模型中。

## 关键技术点
1. 将视觉表示（如图像和点云）直接嵌入语言模型的激活中
2. 使用连续感知输入序列扩展语言模型
3. 保留语言模型的文本生成功能，同时增强其多模态理解能力
4. 规模高达5620亿参数的统一模型

## 实验结果
在多个任务上取得了显著成果：
- 在VQA和OK-VQA等视觉问答任务上超越专用模型
- 展示了对机器人操作和导航的推理能力
- 能够从多个视图中推断3D几何特性
- 证明了模型规模对性能的正面影响

## 影响与应用
PaLM-E开创了将视觉信息直接嵌入大型语言模型的方法，为后续的具身AI研究奠定了基础，特别是对RT-2等VLA模型的发展产生了深远影响。

## 被引情况
- Google Scholar被引量：400+ (2024年6月)
- 近6个月增长曲线：持续上升
- 相关专利数：3+ 