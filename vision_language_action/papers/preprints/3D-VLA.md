# 3D-VLA: A 3D Vision-Language-Action Generative World Model

## 基本信息
- **标题**: 3D-VLA: A 3D Vision-Language-Action Generative World Model
- **作者**: Haoyu Zhen, Xiaowen Qiu, Peihao Chen等
- **发表时间**: 2024
- **会议/期刊**: ICML 2024
- **DOI/链接**: https://arxiv.org/abs/2403.09631
- **代码**: 未公开

## 核心贡献
3D-VLA提出了一种新的具身基础模型，将3D感知、推理和动作通过生成式世界模型无缝连接，解决了传统VLA模型缺乏3D物理世界整合和忽视动作与世界动态关系的问题。

## 关键技术点
1. 基于3D大型语言模型构建，引入交互令牌与具身环境交互
2. 通过训练一系列具身扩散模型并将其与LLM对齐，注入生成能力，预测目标图像和点云
3. 通过从现有机器人数据集中提取大量3D相关信息，创建大规模的3D具身指令数据集

## 实验结果
在多个方面显示出显著改进：
- 在具身环境中显著提高了推理能力
- 增强了多模态生成能力
- 改进了规划能力
- 在实际应用中展示了潜力

## 影响与应用
3D-VLA将VLA模型从2D输入扩展到3D物理世界，同时将世界模型的概念引入VLA领域，为机器人在复杂3D环境中的操作提供了新思路。

## 最新研究状态 [🔥2024]
- 论文在ICML 2024被接收
- 作为2024年具身AI领域的重要突破之一
- 预计将影响未来VLA模型的3D感知和世界建模方向 